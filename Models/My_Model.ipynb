{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K-57-9nVGxtO",
        "outputId": "fb7ce862-062d-4d71-868a-df1e94099396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=1.000 total time=   9.9s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=1.000 total time=  10.2s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=1.000 total time=   8.9s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=1.000 total time=   8.2s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=1.000 total time=   8.5s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=1.000 total time=  16.3s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=1.000 total time=  16.5s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=1.000 total time=  14.2s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=1.000 total time=  13.2s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=1.000 total time=  13.0s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=150;, score=1.000 total time=  19.6s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=150;, score=1.000 total time=  25.6s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=150;, score=1.000 total time=  25.2s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=150;, score=1.000 total time=  25.9s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=150;, score=1.000 total time=  25.4s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=1.000 total time=   8.7s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=1.000 total time=   7.8s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=1.000 total time=   8.8s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=1.000 total time=   8.5s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=1.000 total time=   7.8s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=1.000 total time=  17.3s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=1.000 total time=  16.5s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=1.000 total time=  17.1s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=1.000 total time=  16.7s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=1.000 total time=  16.1s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=150;, score=1.000 total time=  22.7s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=150;, score=1.000 total time=  19.8s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=150;, score=1.000 total time=  19.8s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=150;, score=1.000 total time=  20.2s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=150;, score=1.000 total time=  19.5s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50;, score=1.000 total time=   6.4s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50;, score=1.000 total time=   6.9s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50;, score=1.000 total time=   6.3s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50;, score=1.000 total time=   7.3s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50;, score=1.000 total time=   6.2s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100;, score=1.000 total time=  13.2s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100;, score=1.000 total time=  13.4s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100;, score=1.000 total time=  13.9s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100;, score=1.000 total time=  13.0s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100;, score=1.000 total time=  13.6s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=150;, score=1.000 total time=  19.5s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=150;, score=1.000 total time=  20.5s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=150;, score=1.000 total time=  20.5s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=150;, score=1.000 total time=  19.2s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=150;, score=1.000 total time=  20.3s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=1.000 total time=   6.2s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=1.000 total time=   7.1s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=1.000 total time=   6.7s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=1.000 total time=   7.2s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=1.000 total time=   7.3s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=1.000 total time=  13.6s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=1.000 total time=  13.4s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=1.000 total time=  14.1s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=1.000 total time=  13.6s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=1.000 total time=  13.3s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=150;, score=1.000 total time=  20.0s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=150;, score=1.000 total time=  19.0s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=150;, score=1.000 total time=  20.6s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=150;, score=1.000 total time=  20.1s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=150;, score=1.000 total time=  19.2s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=1.000 total time=   6.9s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=1.000 total time=   6.2s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=1.000 total time=   7.3s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=1.000 total time=   6.2s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=1.000 total time=   7.0s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=1.000 total time=  13.0s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=1.000 total time=  13.1s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=1.000 total time=  14.0s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=1.000 total time=  13.4s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=1.000 total time=  13.4s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=150;, score=1.000 total time=  19.8s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=150;, score=1.000 total time=  20.1s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=150;, score=1.000 total time=  19.9s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=150;, score=1.000 total time=  20.0s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=150;, score=1.000 total time=  19.9s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50;, score=1.000 total time=   6.3s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50;, score=1.000 total time=  10.1s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50;, score=1.000 total time=   6.4s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50;, score=1.000 total time=   6.9s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50;, score=1.000 total time=   6.2s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100;, score=1.000 total time=  13.3s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100;, score=1.000 total time=  13.2s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100;, score=1.000 total time=  13.5s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100;, score=1.000 total time=  13.1s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100;, score=1.000 total time=  13.2s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=150;, score=1.000 total time=  19.1s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=150;, score=1.000 total time=  20.3s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=150;, score=1.000 total time=  20.8s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=150;, score=1.000 total time=  19.3s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=150;, score=1.000 total time=  19.8s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50;, score=1.000 total time=   6.4s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50;, score=1.000 total time=   6.6s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50;, score=1.000 total time=   7.1s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50;, score=1.000 total time=   6.5s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50;, score=1.000 total time=   7.2s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100;, score=1.000 total time=  13.3s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100;, score=1.000 total time=  13.0s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100;, score=1.000 total time=  13.6s\n",
            "[CV 4/5] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100;, score=1.000 total time=  13.0s\n",
            "[CV 5/5] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100;, score=1.000 total time=  13.3s\n",
            "[CV 1/5] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=150;, score=1.000 total time=  19.9s\n",
            "[CV 2/5] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=150;, score=1.000 total time=  18.9s\n",
            "[CV 3/5] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=150;, score=1.000 total time=  20.7s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3b50d0caf7bd>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mbest_rf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"Data4.csv\")\n",
        "\n",
        "# Check for missing values\n",
        "if df.isnull().sum().any():\n",
        "    print(\"Warning: Missing values detected. Consider imputation or removal.\")\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = df.drop(columns=[\"StudentID\", \"Scholarship\"])\n",
        "y = df[\"Scholarship\"]\n",
        "\n",
        "# Encode categorical features\n",
        "X_encoded = pd.get_dummies(X)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_encoded)\n",
        "\n",
        "# Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Apply oversampling\n",
        "oversampler = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(X_scaled, y_encoded)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(rf_model, param_grid, cv=cv, scoring='accuracy',verbose=3)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the model\n",
        "y_train_pred = best_rf_model.predict(X_train)\n",
        "y_test_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "print(\"Training classification report:\")\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "print(\"Testing classification report:\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "# ROC AUC score\n",
        "y_test_prob = best_rf_model.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_test_prob)\n",
        "print(\"ROC AUC score:\", roc_auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"./Data4.csv\")\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = df.drop(columns=[\"StudentID\", \"Scholarship\"])\n",
        "y = df[\"Scholarship\"]\n",
        "\n",
        "# Encode categorical features\n",
        "X_encoded = pd.get_dummies(X)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_encoded)\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode target labels\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Apply oversampling\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(X_scaled, y_encoded)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert target to one-hot encoding\n",
        "y_train_onehot = tf.keras.utils.to_categorical(y_train)\n",
        "y_test_onehot = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Build the TensorFlow model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(13, activation='softmax')  # Assuming 2 classes for Scholarship\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train_onehot,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "train_loss, train_acc = model.evaluate(X_train, y_train_onehot, verbose=0)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=0)\n",
        "\n",
        "print(f\"Training accuracy: {train_acc}\")\n",
        "print(f\"Testing accuracy: {test_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9ByWtfrIqd1",
        "outputId": "135c4234-1723-40ad-ce34-4e3d3fab54ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5080/5080 [==============================] - 13s 2ms/step - loss: 0.7503 - accuracy: 0.7435 - val_loss: 0.1613 - val_accuracy: 0.9414\n",
            "Epoch 2/50\n",
            "5080/5080 [==============================] - 13s 3ms/step - loss: 0.3094 - accuracy: 0.8955 - val_loss: 0.0862 - val_accuracy: 0.9691\n",
            "Epoch 3/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.2242 - accuracy: 0.9241 - val_loss: 0.0665 - val_accuracy: 0.9753\n",
            "Epoch 4/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1929 - accuracy: 0.9357 - val_loss: 0.0537 - val_accuracy: 0.9798\n",
            "Epoch 5/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1718 - accuracy: 0.9428 - val_loss: 0.0484 - val_accuracy: 0.9826\n",
            "Epoch 6/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1613 - accuracy: 0.9464 - val_loss: 0.0454 - val_accuracy: 0.9838\n",
            "Epoch 7/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1512 - accuracy: 0.9494 - val_loss: 0.0406 - val_accuracy: 0.9870\n",
            "Epoch 8/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1465 - accuracy: 0.9516 - val_loss: 0.0391 - val_accuracy: 0.9863\n",
            "Epoch 9/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1420 - accuracy: 0.9537 - val_loss: 0.0359 - val_accuracy: 0.9880\n",
            "Epoch 10/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1387 - accuracy: 0.9551 - val_loss: 0.0362 - val_accuracy: 0.9876\n",
            "Epoch 11/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1333 - accuracy: 0.9564 - val_loss: 0.0361 - val_accuracy: 0.9872\n",
            "Epoch 12/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1304 - accuracy: 0.9573 - val_loss: 0.0324 - val_accuracy: 0.9884\n",
            "Epoch 13/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1280 - accuracy: 0.9584 - val_loss: 0.0303 - val_accuracy: 0.9895\n",
            "Epoch 14/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1270 - accuracy: 0.9588 - val_loss: 0.0311 - val_accuracy: 0.9885\n",
            "Epoch 15/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1248 - accuracy: 0.9596 - val_loss: 0.0321 - val_accuracy: 0.9892\n",
            "Epoch 16/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1227 - accuracy: 0.9601 - val_loss: 0.0282 - val_accuracy: 0.9906\n",
            "Epoch 17/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1219 - accuracy: 0.9611 - val_loss: 0.0280 - val_accuracy: 0.9903\n",
            "Epoch 18/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1199 - accuracy: 0.9620 - val_loss: 0.0318 - val_accuracy: 0.9889\n",
            "Epoch 19/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1185 - accuracy: 0.9621 - val_loss: 0.0296 - val_accuracy: 0.9898\n",
            "Epoch 20/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1180 - accuracy: 0.9632 - val_loss: 0.0289 - val_accuracy: 0.9901\n",
            "Epoch 21/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1177 - accuracy: 0.9625 - val_loss: 0.0373 - val_accuracy: 0.9861\n",
            "Epoch 22/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1158 - accuracy: 0.9628 - val_loss: 0.0246 - val_accuracy: 0.9929\n",
            "Epoch 23/50\n",
            "5080/5080 [==============================] - 10s 2ms/step - loss: 0.1158 - accuracy: 0.9630 - val_loss: 0.0294 - val_accuracy: 0.9897\n",
            "Epoch 24/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1136 - accuracy: 0.9637 - val_loss: 0.0277 - val_accuracy: 0.9901\n",
            "Epoch 25/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1135 - accuracy: 0.9637 - val_loss: 0.0273 - val_accuracy: 0.9904\n",
            "Epoch 26/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1125 - accuracy: 0.9644 - val_loss: 0.0272 - val_accuracy: 0.9898\n",
            "Epoch 27/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1116 - accuracy: 0.9649 - val_loss: 0.0255 - val_accuracy: 0.9907\n",
            "Epoch 28/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1111 - accuracy: 0.9644 - val_loss: 0.0252 - val_accuracy: 0.9915\n",
            "Epoch 29/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1092 - accuracy: 0.9659 - val_loss: 0.0256 - val_accuracy: 0.9894\n",
            "Epoch 30/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1110 - accuracy: 0.9654 - val_loss: 0.0262 - val_accuracy: 0.9909\n",
            "Epoch 31/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1062 - accuracy: 0.9662 - val_loss: 0.0290 - val_accuracy: 0.9892\n",
            "Epoch 32/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1067 - accuracy: 0.9663 - val_loss: 0.0224 - val_accuracy: 0.9932\n",
            "Epoch 33/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1043 - accuracy: 0.9676 - val_loss: 0.0248 - val_accuracy: 0.9914\n",
            "Epoch 34/50\n",
            "5080/5080 [==============================] - 10s 2ms/step - loss: 0.1082 - accuracy: 0.9659 - val_loss: 0.0255 - val_accuracy: 0.9911\n",
            "Epoch 35/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1078 - accuracy: 0.9665 - val_loss: 0.0288 - val_accuracy: 0.9897\n",
            "Epoch 36/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1040 - accuracy: 0.9675 - val_loss: 0.0215 - val_accuracy: 0.9938\n",
            "Epoch 37/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1067 - accuracy: 0.9672 - val_loss: 0.0228 - val_accuracy: 0.9924\n",
            "Epoch 38/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1057 - accuracy: 0.9677 - val_loss: 0.0224 - val_accuracy: 0.9927\n",
            "Epoch 39/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1040 - accuracy: 0.9674 - val_loss: 0.0240 - val_accuracy: 0.9918\n",
            "Epoch 40/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1029 - accuracy: 0.9682 - val_loss: 0.0247 - val_accuracy: 0.9920\n",
            "Epoch 41/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1048 - accuracy: 0.9677 - val_loss: 0.0211 - val_accuracy: 0.9926\n",
            "Epoch 42/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.1052 - accuracy: 0.9684 - val_loss: 0.0248 - val_accuracy: 0.9916\n",
            "Epoch 43/50\n",
            "5080/5080 [==============================] - 11s 2ms/step - loss: 0.0998 - accuracy: 0.9690 - val_loss: 0.0257 - val_accuracy: 0.9910\n",
            "Epoch 44/50\n",
            "5080/5080 [==============================] - 10s 2ms/step - loss: 0.0991 - accuracy: 0.9692 - val_loss: 0.0221 - val_accuracy: 0.9925\n",
            "Epoch 45/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1044 - accuracy: 0.9682 - val_loss: 0.0218 - val_accuracy: 0.9927\n",
            "Epoch 46/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.0993 - accuracy: 0.9692 - val_loss: 0.0200 - val_accuracy: 0.9935\n",
            "Epoch 47/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.0988 - accuracy: 0.9692 - val_loss: 0.0234 - val_accuracy: 0.9918\n",
            "Epoch 48/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.0998 - accuracy: 0.9697 - val_loss: 0.0218 - val_accuracy: 0.9925\n",
            "Epoch 49/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.1000 - accuracy: 0.9695 - val_loss: 0.0216 - val_accuracy: 0.9930\n",
            "Epoch 50/50\n",
            "5080/5080 [==============================] - 12s 2ms/step - loss: 0.0986 - accuracy: 0.9701 - val_loss: 0.0209 - val_accuracy: 0.9930\n",
            "Training accuracy: 0.99374920129776\n",
            "Testing accuracy: 0.9934834837913513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_encoded)"
      ],
      "metadata": {
        "id": "Y1q2PT00W__J"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model to a HDF5 file\n",
        "model.save('my_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5C91xLYXYYZ",
        "outputId": "7c269067-44ac-4555-b468-4b032fdbdf4f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFFlj7DzaeNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ],
      "metadata": {
        "id": "3Aw6_He_ONGR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Function to preprocess input data\n",
        "def preprocess_input(input_dict):\n",
        "\n",
        "    myList = []\n",
        "    x = input_dict\n",
        "    required_list = ['GPA', '10th Percentage', '12th Percentage', 'Family Income', 'Extracurricular Activities_high', 'Extracurricular Activities_low', 'Extracurricular Activities_medium', 'Essay Quality_excellent', 'Essay Quality_fair', 'Essay Quality_good', 'Essay Quality_poor', 'Letters of Recommendation_moderate', 'Letters of Recommendation_strong', 'Letters of Recommendation_weak', 'Financial Need_high', 'Financial Need_low', 'Financial Need_medium', 'Major_Arts', 'Major_Business', 'Major_Engineering', 'Major_Medicine', 'Major_Science', 'State of Residence_Delhi', 'State of Residence_Karnataka', 'State of Residence_Kerala', 'State of Residence_Maharashtra', 'State of Residence_Tamil Nadu', 'State of Residence_Uttar Pradesh', 'Leadership Experience_no', 'Leadership Experience_yes', 'Volunteer Work_no', 'Volunteer Work_yes', 'Work Experience_no', 'Work Experience_yes', 'Family Background_high', 'Family Background_low', 'Family Background_medium']\n",
        "    for cols in required_list:\n",
        "      if cols=='GPA':\n",
        "        myList.append(x['GPA'])\n",
        "      elif cols=='10th Percentage':\n",
        "        myList.append(x['10th Percentage'])\n",
        "      elif cols=='12th Percentage':\n",
        "        myList.append(x['12th Percentage'])\n",
        "      elif cols=='Family Income':\n",
        "        myList.append(x['Family Income'])\n",
        "      elif cols.split(\"_\")[0] in x:\n",
        "        #print(cols)\n",
        "        #print(cols.split(\"_\")[1])\n",
        "        if cols.split(\"_\")[1]==x[cols.split(\"_\")[0]]:\n",
        "          myList.append(True)\n",
        "        else:\n",
        "          myList.append(False)\n",
        "\n",
        "\n",
        "\n",
        "    input_df_scaled = scaler.transform([myList])\n",
        "\n",
        "    return input_df_scaled\n",
        "\n",
        "# Trained scaler from the training data\n",
        "\n",
        "\n",
        "# Preprocess the input data\n",
        "input_data = {\n",
        "    'GPA': '10',\n",
        "    '10th Percentage': '90',\n",
        "    '12th Percentage': '96',\n",
        "    'Family Income': '100000',\n",
        "    'Extracurricular Activities': 'high',\n",
        "    'Essay Quality': 'poor',\n",
        "    'Letters of Recommendation': 'weak',\n",
        "    'Financial Need': 'low',\n",
        "    'Leadership Experience': 'no',\n",
        "    'Volunteer Work': 'no',\n",
        "    'Work Experience': 'yes',\n",
        "    'Family Background': 'medium',\n",
        "    'Major':'Engineering',\n",
        "    'State of Residence': 'Karnataka'\n",
        "\n",
        "}\n",
        "\n",
        "input_data_scaled = preprocess_input(input_data)\n",
        "\n",
        "# Load the trained TensorFlow model\n",
        "model = tf.keras.models.load_model('my_model.h5')  # Replace 'your_model_path' with the path to your trained model\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(input_data_scaled)\n",
        "\n",
        "# Convert predictions to class labels\n",
        "predicted_class = np.argmax(predictions, axis=1)[0]  # Assuming binary classification\n",
        "\n",
        "# Inverse transform using LabelEncoder\n",
        "predicted_class_label = label_encoder.inverse_transform([predicted_class])[0]\n",
        "\n",
        "print(f\"Predicted Scholarship: {predicted_class_label}\")\n",
        "\n",
        "# Get the test set true labels for confusion matrix\n",
        "# Assuming X_test and y_test are your test set and true labels\n",
        "# y_test_labels = label_encoder.inverse_transform(y_test)\n",
        "\n",
        "# For demonstration, let's assume y_test_labels contains the true labels for the test set\n",
        "\n",
        "# Create confusion matrix\n",
        "# conf_matrix = confusion_matrix(y_test_labels, model.predict_classes(X_test))  # Replace with your actual test set and true labels\n",
        "# sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Not Scholarship', 'Scholarship'], yticklabels=['Not Scholarship', 'Scholarship'])\n",
        "# plt.ylabel('Actual')\n",
        "# plt.xlabel('Predicted')\n",
        "# plt.title('Confusion Matrix')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mahacFEiV4iZ",
        "outputId": "ead27b84-59a3-48ba-a973-51bd6a27b567"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 87ms/step\n",
            "Predicted Scholarship: Post-Graduate Indira Gandhi Scholarship - 36,200 per annum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('scaler_tensorflow.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)"
      ],
      "metadata": {
        "id": "OquJxPT-WqLl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)"
      ],
      "metadata": {
        "id": "nUZRu924al39"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "myList = []\n",
        "input_data = {\n",
        "    'GPA': '9.77',\n",
        "    '10th Percentage': '61',\n",
        "    '12th Percentage': '86',\n",
        "    'Family Income': '85643',\n",
        "    'Extracurricular Activities': 'low',\n",
        "    'Essay Quality': 'poor',\n",
        "    'Letters of Recommendation': 'moderate',\n",
        "    'Financial Need': 'medium',\n",
        "    'Leadership Experience': 'yes',\n",
        "    'Volunteer Work': 'yes',\n",
        "    'Work Experience': 'yes',\n",
        "    'Family Background': 'low',\n",
        "    'Major':'Engineering',\n",
        "    'State of Residence': 'Kerela'\n",
        "\n",
        "}\n",
        "x = input_data\n",
        "required_list = ['GPA', '10th Percentage', '12th Percentage', 'Family Income', 'Extracurricular Activities_high', 'Extracurricular Activities_low', 'Extracurricular Activities_medium', 'Essay Quality_excellent', 'Essay Quality_fair', 'Essay Quality_good', 'Essay Quality_poor', 'Letters of Recommendation_moderate', 'Letters of Recommendation_strong', 'Letters of Recommendation_weak', 'Financial Need_high', 'Financial Need_low', 'Financial Need_medium', 'Major_Arts', 'Major_Business', 'Major_Engineering', 'Major_Medicine', 'Major_Science', 'State of Residence_Delhi', 'State of Residence_Karnataka', 'State of Residence_Kerala', 'State of Residence_Maharashtra', 'State of Residence_Tamil Nadu', 'State of Residence_Uttar Pradesh', 'Leadership Experience_no', 'Leadership Experience_yes', 'Volunteer Work_no', 'Volunteer Work_yes', 'Work Experience_no', 'Work Experience_yes', 'Family Background_high', 'Family Background_low', 'Family Background_medium']\n",
        "for cols in required_list:\n",
        "  if cols=='GPA':\n",
        "    myList.append(x['GPA'])\n",
        "  elif cols=='10th Percentage':\n",
        "    myList.append(x['10th Percentage'])\n",
        "  elif cols=='12th Percentage':\n",
        "    myList.append(x['12th Percentage'])\n",
        "  elif cols=='Family Income':\n",
        "    myList.append(x['Family Income'])\n",
        "  elif cols.split(\"_\")[0] in x:\n",
        "    #print(cols)\n",
        "    #print(cols.split(\"_\")[1])\n",
        "    if cols.split(\"_\")[1]==x[cols.split(\"_\")[0]]:\n",
        "      myList.append(True)\n",
        "    else:\n",
        "      myList.append(False)\n",
        "with open('scaler_tensorflow.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "with open('label_encoder_tensorflow.pkl', 'rb') as f:\n",
        "    label_encode = pickle.load(f)\n",
        "\n",
        "input_df_scaled = scaler.transform([myList])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3KtRZ33ZZ8W",
        "outputId": "7251a4d4-23a8-4449-97b6-75a4cd70b3cb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(input_data_scaled)\n",
        "predicted_class_label = label_encode.inverse_transform([predicted_class])[0]\n",
        "print(predicted_class_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U23VkafaDvU",
        "outputId": "006c6ce4-6da8-4c21-c0c7-b64007201504"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n",
            "Post-Graduate Indira Gandhi Scholarship - 36,200 per annum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l40daBgaH6U",
        "outputId": "f952e66a-1957-4f01-af1e-0635ac804816"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUdr1J7tbrmk",
        "outputId": "242c94af-10bd-48ad-e60e-c342a4ec6b14"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sxUmNloeT9B",
        "outputId": "cffe7f9d-3c49-4cf2-9107-56a43932058d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZtaMtx8ygcgg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}